<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Warum</title>
  <link rel="stylesheet" href="style.css">
  <style>
    .topics a {
      color: white;
    }
  </style>
</head>
<body class="dark">
  <div id="header-placeholder"></div>

  <main>
    <section class="hero">
      <h1>Mit welchem Ziel werden KI-FakeNews verbreitet?</h1>
      <p> </p>
    </section>

    <section class="topics">
      <ol class="main-list">
        <li>
          <h2 class="group-title">Vielfältige Motivationen hinter KI-Fehlinformation</h2>
          <p>Künstliche Intelligenz hat sich zu einer mächtigen Waffe in den Händen von Bösewichten entwickelt, die gezielt Fehlinformationen verbreiten. Die Motivation dahinter ist vielfältig und reicht von politischer Manipulation über finanziellen Betrug bis hin zum strategischen Einsatz in Konflikten.</p>
        </li>

        <li>
          <h2 class="group-title">Politische Manipulation durch Deepfakes</h2>
          <p>Im Zentrum steht dabei häufig die Erreichung gesellschaftlicher, politischer oder finanzieller Ziele durch gezielte Manipulation der öffentlichen Meinung. Ein Deepfake aus dem Frühjahr 2022 zeigt Vladimir Zelenskiy, wie er seine Truppen auffordert, die Waffen niederzulegen und nach Hause zu ihren Familien zurückzukehren, er würde ebenfalls die Donbas-Region an Russland überreichen (<a href="https://www.youtube.com/watch?v=X17yrEV5sl4" target="_blank">YouTube</a>). In Großbritannien wurde einer Lehrerin vorgeworfen, beim Haustürwahlkampf rassistische Beleidigungen von sich zu geben, was mit einem Überwachungsvideo mit gefälschtem Ton belegt wurde (<a href="https://www.theguardian.com/education/2025/jan/19/teacher-was-forced-into-hiding-after-fake-video-appeared-to-show-her-making-racist-slur" target="_blank">The Guardian</a>). Auf der Plattform X kursierte ein Deepfake-Video, in dem Kamala Harris über Joe Bidens "Senilität" lästerte und sich als "ultimativen Diversity-Hire" bezeichnete - ein Video, das sogar Elon Musk ohne Kennzeichnung weiterverbreitete (<a href="https://youtu.be/NIz-aoZFzM4?si=a5elA46SpALU78Ta" target="_blank">YouTube</a>).</p>
        </li>

        <li>
          <h2 class="group-title">Finanzielle Ausbeutung durch Voice Cloning</h2>
          <p>Besonders perfide ist der Einsatz von KI im Betrugsbereich, wo vor allem ältere Menschen zur Zielgruppe werden. Mit sogenanntem Voice Cloning benötigen Kriminelle oft nur wenige Sekunden einer Sprachaufnahme aus sozialen Medien, um die Stimme von Familienmitgliedern täuschend echt zu imitieren. Die Betrüger rufen dann ihre Opfer an und schildern dramatische Szenarien wie Unfälle oder Festnahmen, um schnelle finanzielle Hilfe zu erbitten. Diese moderne Variante des Enkeltricks ist besonders gefährlich, weil die vertraute Stimme des vermeintlichen Enkels oder der Enkelin enormen emotionalen Druck aufbaut. In Hongkong wurde ein multinationales Unternehmen im Februar 2024 um umgerechnet etwa 23 Millionen Euro geprellt, als ein Angestellter bei einer Videokonferenz getäuscht wurde, bei der die meisten Teilnehmer KI-generierte Nachbildungen waren (<a href="https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk" target="_blank">CNN</a>).</p>
        </li>

        <li>
          <h2 class="group-title">Massenhafte Skalierbarkeit von Desinformation</h2>
          <p>Ein entscheidender Faktor für die Verbreitung von KI-Fehlinformation ist ihre Skalierbarkeit. Soziale Netzwerke können in kürzester Zeit mit manipulierten Inhalten geflutet werden - schneller, als authentische, entlarvende Inhalte produziert werden können. Was früher Stunden technischer Arbeit erforderte, lässt sich heute in Sekunden umsetzen: Ein einzelner Nutzer kann im Handumdrehen dutzende vermeintliche Nachrichtenmeldungen oder manipulierte Bilder erstellen. Diese industrielle Dimension der Desinformation macht es nahezu unmöglich, alle Fälschungen rechtzeitig zu identifizieren und zu korrigieren, bevor sie ihre Wirkung entfalten.</p>
        </li>

        <li>
          <h2 class="group-title">Systematischer Vertrauensverlust in Medien</h2>
          <p>Die Gefahr liegt nicht nur in einzelnen erfolgreichen Täuschungen, sondern im systematischen Vertrauensverlust: Je häufiger Deepfakes bekannt werden, desto schneller zweifeln Menschen an der Echtheit von Fotos, Videos oder Tonaufnahmen – selbst wenn diese tatsächlich authentisch sind.</p>
        </li>
      </ol>
    </section>
  </main>

  <script src="script.js"></script>
</body>
</html>